{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This tutorial is adapted from [Damir Cavar](http://damir.cavar.me/) and Callie Federer tutorial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perceptron is simple model that can help us solve such a problem given enough data. **The perceptron can be defined as an algorithm for supervised binary classification.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine we have to predict whether your friend, Mbali, would be going for a workout or not. We have the following things to take into account:\n",
    "\n",
    "- #### Weather (Sunny or Rainy) ?\n",
    "- #### Time of Day (Morning or Evening) ?\n",
    "- #### Energy Level (Energized) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/perceptron_cartoon.png\" style=\"max-width:100%; width: 70%; max-width: none\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons are made up of three components, namely:\n",
    "\n",
    "    - The input: What we serve the perceptron with for inference\n",
    "    - The input weights: Our associated weighting for each input feature\n",
    "    - The activation function: A binary threshold function (Fire or No Fire!)\n",
    "    \n",
    "Define perceptron mathematically here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: To Gym or Not To Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import *numpy* and define our *activation* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bactivation(z):\n",
    "    if z == 0.5:\n",
    "        return 1\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define our example data **input** $x$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = 0\n",
    "evening = 1\n",
    "energized = 1\n",
    "x = np.array([weather, evening, energized])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we define the corresponding **weights** $w$, and **bias** $b$ of our perceptron $p$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([0.5, 0.5, 0])\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our perceptron would compute $p$ as the **dot-product** $w \\cdot x$ and add the **bias** $b$ to it. Subsequently, the activation function defined above will convert this $p$ value to the **activation value** $a$ of the unit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = w.dot(x) + b\n",
    "a = bactivation(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Value (p): 0.5\n",
      "Activation Value of Perceptron (a): 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Perceptron Value (p):\", p)\n",
    "print(\"Activation Value of Perceptron (a):\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Assignments\n",
    "\n",
    "Provide an instance $x$ to the perceptron $p$, where your friend, Mbali, feels energized, but it's the evening and it's raining?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic Gates (AND, OR) with Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons are thought to be good function approximators for Logic Gates. In this mini-tutorial, we will fit logic gates, AND & OR, with a perceptron!\n",
    "\n",
    "| x1 | x2  | AND | OR |\n",
    "| --- | ------ |:------:| ---- |\n",
    "| 0 | 0  | 0 | 0 |\n",
    "| 0 | 1  | 0 | 1 |\n",
    "| 1 | 0  | 0 | 1 |\n",
    "| 1 | 1  | 1 | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to implement a simple **perceptron** to compute logical operations like AND, and OR.\n",
    "\n",
    "- Input: $x_1$ and $x_2$\n",
    "- Bias: $b = -1$ for AND; $b = 0$ for OR\n",
    "- Weights: $w = [1, 1]$\n",
    "\n",
    "with the following activation function:\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "    \\ 0 & \\quad \\text{if } w \\cdot x + b \\leq 0\\\\\n",
    "    \\ 1 & \\quad \\text{if } w \\cdot x + b > 0\n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define this threshold function in Python as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(z):\n",
    "    if z > 0:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0]\n",
    "    ,[1,0]\n",
    "    ,[0,1]\n",
    "    ,[1,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For AND we could implement a perceptron as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AND 0: 0\n",
      "1 AND 0: 0\n",
      "0 AND 1: 0\n",
      "1 AND 1: 1\n"
     ]
    }
   ],
   "source": [
    "w = np.array([1, 1])\n",
    "b = -1\n",
    "\n",
    "for x in X:\n",
    "    a = activation(w.dot(x) + b)\n",
    "    print(f'{x[0]} AND {x[1]}: {a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For OR we could implement a perceptron as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 OR 0: 0\n",
      "1 OR 0: 1\n",
      "0 OR 1: 1\n",
      "1 OR 1: 1\n"
     ]
    }
   ],
   "source": [
    "w = np.array([1, 1])\n",
    "b = 0\n",
    "x = np.array([0, 0])\n",
    "\n",
    "for x in X:\n",
    "    a = activation(w.dot(x) + b)\n",
    "    print(f'{x[0]} OR {x[1]}: {a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Assignment\n",
    "\n",
    "Can you implement a perceptron for 3 logic gates for the NOR operation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Perceptrons: Gradient Descent and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update the weights of a perceptron $p$ given some data $x$ and loss function $L$, we use ***Gradient Descent***.\n",
    "\n",
    "***Gradient Descent*** is a first-order gradient optimization algorithm. At first, the weights are randomly initialized, then the optimization algorithm takes repeated steps in the opposite direction of the gradient until it reaches a \"satisfactory condition\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SGD](resources/sgd.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent equation can be encapsulated in the following equation:\n",
    "\n",
    "$$\n",
    "w_{k+1} = w_{k} - \\alpha \\nabla L_{w}\n",
    "$$\n",
    "\n",
    ",where $L$ is the loss function and $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Backpropagation*** computes the gradient of a loss function with respect to the weights of the network $\\nabla L_{w}$ for a single inputâ€“output example, and does so efficiently, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule.\n",
    "\n",
    "Let's first define a simple loss function $L(w)$ for a perceptron $p$:\n",
    "\n",
    "$$\n",
    "L(w) = (target - y)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = \\sigma( \\sum_k^{N} w_k x_k + b) \n",
    "$$\n",
    "\n",
    "Compute the gradient for a perceptron, takes the following shape:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_i} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial \\sigma} \\frac{\\partial \\sigma}{\\partial w_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we fit a simple logic problem. We want to make the following predictions from the input:\n",
    "\n",
    "| x1 | x2 | x3 | Output |\n",
    "| - | - | - |:------:|\n",
    "| 0 | 0 | 1  | 0      |\n",
    "| 1 | 1 | 1  | 1      |\n",
    "| 1 | 0 | 1  | 1      |\n",
    "| 0 | 1 | 1  | 0      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use *Numpy* to compute the network parameters, weights, activation, and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the *[Sigmoid](http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#sigmoid)* activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use the [ReLU](http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#activation-relu) activation function instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \"\"\"The ReLU activation function.\"\"\"\n",
    "    return max(0, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Sigmoid](http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#sigmoid) activation function introduces non-linearity to the computation. It maps the input value to an output value between $0$ and $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/SigmoidFunction1.png\" style=\"max-width:100%; width: 30%; max-width: none\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the sigmoid function is maximal at $x=0$ and minimal for lower or higher values of $x$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/sigmoid_prime.png\" style=\"max-width:100%; width: 25%; max-width: none\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *sigmoid_prime* function returns the derivative of the sigmoid for any given $z$. The derivative of the sigmoid is $z * (1 - z)$. This is basically the slope of the sigmoid function at any given point: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    \"\"\"The derivative of sigmoid for z.\"\"\"\n",
    "    return z * (1 - z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Assignment\n",
    "\n",
    "What is the derivative of the ReLU activation function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the inputs as rows in *X*. There are three input nodes (three columns per vector in $X$. Each row is one trainig example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([ [ 0, 0, 1 ],\n",
    "               [ 0, 1, 1 ],\n",
    "               [ 1, 0, 1 ],\n",
    "               [ 1, 1, 1 ] ])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs are stored in *y*, where each row represents the output for the corresponding input vector (row) in *X*. The vector is initiated as a single row vector and with four columns and transposed (using the $.T$ method) into a column vector with four rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "y = np.array([[0,0,1,1]]).T\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the outputs deterministic, we seed the random number generator with a constant. This will guarantee that every time you run the code, you will get the same random distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our perceptron, we create a weight matrix ($Wo$) with randomly initialized weights:\n",
    "\n",
    "TODO: Delve into how we initialize the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.43532763]\n",
      " [0.5649153 ]\n",
      " [0.25761743]]\n"
     ]
    }
   ],
   "source": [
    "n_inputs = 3\n",
    "n_outputs = 1\n",
    "Wo = np.random.random( (n_inputs, n_outputs) ) * np.sqrt(2.0/n_inputs)\n",
    "print(Wo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for the output weight matrix ($Wo$) to have 3 rows and 1 column is that it represents the weights of the connections from the three input neurons to the single output neuron. The initialization of the weight matrix is random with a mean of $0$ and a variance of $1$. There is a good reason for chosing a mean of zero in the weight initialization. See for details the section on Weight Initialization in the [Stanford course CS231n on Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/neural-networks-2/#init).\n",
    "\n",
    "\n",
    "The core representation of this network is basically the weight matrix *Wo*. The rest, input matrix, output vector and so on are components that we need for learning and evaluation. The learning result is stored in the *Wo* weight matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop in the optimization and learning cycle 10,000 times. In the *forward propagation* line we process the entire input matrix for training. This is called **full batch** training. I do not use an alternative variable name to represent the input layer, instead I use the input matrix $X$ directly here. Think of this as the different inputs to the input neurons computed at once. In principle the input or training data could have many more training examples, the code would stay the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the result of the perceptron without training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56405051],\n",
       "       [0.6947737 ],\n",
       "       [0.66662175],\n",
       "       [0.77865756]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.dot(X, Wo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recall that the loss function is the squared error loss and using the perceptron gradient equation, our backpropagation equation would translate to:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_i} = [ - 2 * (target - y)] * [\\sigma'(\\sum_k^{N} w_k x_k + b)] * [x_i] \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training with this gradient update equation, we will improve error on our perceptron model. \n",
    "\n",
    "Observe the error of our initialized perceptron model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31815298],\n",
       "       [0.4827105 ],\n",
       "       [0.11114106],\n",
       "       [0.04899248]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y - sigmoid(np.dot(X, Wo)))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(10000):\n",
    "    \n",
    "    # forward propagation\n",
    "    prediction = sigmoid(np.dot(X, Wo))\n",
    "    \n",
    "    # compute the loss\n",
    "    loss_gradient = -2 * (y - prediction)\n",
    "    \n",
    "    # multiply the loss by the slope of the sigmoid at l1 (Backpropagation Step)\n",
    "    l1_delta = sigmoid_prime(prediction) * loss_gradient\n",
    "    gradients = np.dot(X.T, l1_delta)\n",
    "    \n",
    "    # update weights (Gradient Descent)\n",
    "    Wo += - gradients\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now observe the error of our trained perceptron model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.61951671e-05],\n",
       "       [3.06376089e-05],\n",
       "       [2.03748845e-05],\n",
       "       [3.07354915e-05]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y - sigmoid(np.dot(X, Wo)))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the result of the perceptron with training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0067967 ],\n",
       "       [0.00553513],\n",
       "       [0.99548615],\n",
       "       [0.99445604]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.dot(X, Wo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Famous XOR Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of neural units comes from combining them into larger networks. Minsky and Papert (1969): A single neural unit cannot compute the simple logical function XOR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this narrow definition of a perceptron, it seems not possible to implement an XOR logic perceptron. The restriction is that there is a threshold function that is binary and piecewise linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one student in my 2020 L645 class, Kazuki Yabe, points out, with a different activation function and a different weight vector, one unit can of course handle XOR. If we use the following activation function:\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "    \\ 0 & \\quad \\text{if } w \\cdot x + b \\neq 0.5\\\\\n",
    "    \\ 1 & \\quad \\text{if } w \\cdot x + b = 0.5\n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bactivation(z):\n",
    "    if z == 0.5:\n",
    "        return 1\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume the weights to be set to 0.5 and the bias to 0, one unit can handle the XOR logic:\n",
    "\n",
    "- Input: $x_1$ and $x_2$\n",
    "- Bias: $b = 0$ for XOR\n",
    "- Weights: $w = [0.5, 0.5]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 OR 0: 0\n",
      "1 OR 0: 1\n",
      "0 OR 1: 1\n",
      "1 OR 1: 0\n"
     ]
    }
   ],
   "source": [
    "w = np.array([0.5, 0.5])\n",
    "b = 0\n",
    "x = np.array([0, 0])\n",
    "print(\"0 OR 0:\", bactivation(w.dot(x) + b))\n",
    "x = np.array([1, 0])\n",
    "print(\"1 OR 0:\", bactivation(w.dot(x) + b))\n",
    "x = np.array([0, 1])\n",
    "print(\"0 OR 1:\", bactivation(w.dot(x) + b))\n",
    "x = np.array([1, 1])\n",
    "print(\"1 OR 1:\", bactivation(w.dot(x) + b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular activation function is of course not differentiable, and it remains to be shown that the weights can be learned, but nevertheless, a single unit can be identified that solves the XOR problem.\n",
    "\n",
    "The difference between Minsky and Papert's (1969) definition of a perceptron and this unit is that - as Julia Hockenmaier pointed out - a perceptron is defined to have a decision function that would be binary and piecewise linear. This means that the unit that solves the XOR problem is not compatible with the definition of perceptron as in Minsky and Papert (1969) (p.c. Julia Hockenmaier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisiting Minsky and Papert (1969): The Tri-Perceptron XOR Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a proposed solution in [Goodfellow et al. (2016)](https://www.deeplearningbook.org/) for the XOR problem, using a network with two layers of ReLU-based units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![XOR Network](resources/XOR_Network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This two layer and three perceptron network solves the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more deiscussion on this problem, consult:\n",
    "\n",
    "- [Wikipedia on the XOR problem](https://en.wikipedia.org/wiki/Perceptron)\n",
    "- [Solving XOR with a single Perceptron](https://medium.com/@lucaspereira0612/solving-xor-with-a-single-perceptron-34539f395182)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Multi-Layer Perceptron is a model which consists of at least three layers of perceptrons: input layer, hidden layer, and output layer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLP](resources/mlp_hidden.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would backpropagation work in such a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLP_backprop](resources/mlp_backprop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following dataset:\n",
    "\n",
    "| Input  | Output |\n",
    "| ------ |:------:|\n",
    "| 0 0 1  | 0      |\n",
    "| 0 1 1  | 1      |\n",
    "| 1 0 1  | 1      |\n",
    "| 1 1 1  | 0      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern here is a XOR pattern problem: If there is a $1$ in either column $1$ or $2$, but not in both, the output is $1$ (XOR over column $1$ and $2$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this problem, we need a network with another layer (MLP), that is a layer that will combine and transform the input, and an additional layer will map it to the output. We will add a *hidden layer* with randomized weights and then train those to optimize the output probabilities of the table above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a new $X$ input matrix that reflects the above table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0, 1],\n",
    "              [0, 1, 1],\n",
    "              [1, 0, 1],\n",
    "              [1, 1, 1]])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a new output matrix $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "y = np.array([[ 0, 1, 1, 0]]).T\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the random number generator with a constant again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that our 3 inputs are mapped to 4 hidden layer ($Wh$) neurons, we have to initialize the hidden layer weights in a 3 by 4 matrix. The outout layer ($Wo$) is a single neuron that is connected to the hidden layer, thus the output layer is a 4 by 1 matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wh:\n",
      " [[3.40497041e-01 5.88142486e-01 9.33866473e-05 2.46853512e-01]\n",
      " [1.19825683e-01 7.53941469e-02 1.52080826e-01 2.82149152e-01]\n",
      " [3.23959286e-01 4.39942021e-01 3.42270888e-01 5.59479379e-01]]\n",
      "Wo:\n",
      " [[0.14456957]\n",
      " [0.62092279]\n",
      " [0.01936595]\n",
      " [0.47409212]]\n"
     ]
    }
   ],
   "source": [
    "n_inputs = 3\n",
    "n_hidden_neurons = 4\n",
    "n_output_neurons = 1\n",
    "Wh = np.random.random( (n_inputs, n_hidden_neurons) )  * np.sqrt(2.0/n_inputs)\n",
    "Wo = np.random.random( (n_hidden_neurons, n_output_neurons) )  * np.sqrt(2.0/n_hidden_neurons)\n",
    "print(\"Wh:\\n\", Wh)\n",
    "print(\"Wo:\\n\", Wo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will loop now 100,000 times to optimize the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.49962354308293877\n",
      "Error: 0.010185068686370102\n",
      "Error: 0.006740755437297383\n",
      "Error: 0.005345052957883491\n",
      "Error: 0.004547502999833713\n",
      "Error: 0.004017450105072937\n",
      "Error: 0.0036334495140870095\n",
      "Error: 0.003339229754058863\n",
      "Error: 0.003104745370932276\n",
      "Error: 0.002912327550144696\n",
      "Wo:\n",
      " [[-7.07355497]\n",
      " [14.20975817]\n",
      " [-7.01708617]\n",
      " [-7.15528258]]\n",
      "Wh:\n",
      " [[ 6.66182717  6.80494448 -3.00320286  3.65642155]\n",
      " [-3.43419418  6.77518798  6.58168773  3.71366194]\n",
      " [ 0.59089835 -1.96151177  0.18071609 -5.62078292]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(100000):\n",
    "    l1 = sigmoid(np.dot(X, Wh))\n",
    "    l2 = sigmoid(np.dot(l1, Wo))\n",
    "    \n",
    "    l2_error = y - l2\n",
    "    \n",
    "    if (i % 10000) == 0:\n",
    "        print(\"Error:\", np.mean(np.abs(l2_error)))\n",
    "    \n",
    "    # gradient, changing towards the target value\n",
    "    l2_delta = l2_error * sigmoid_prime(l2)\n",
    "    \n",
    "    # compute the l1 contribution by value to the l2 error, given the output weights\n",
    "    l1_error = l2_delta.dot(Wo.T)\n",
    "    \n",
    "    # direction of the l1 target:\n",
    "    # in what direction is the target l1?\n",
    "    l1_delta = l1_error * sigmoid_prime(l1)\n",
    "    \n",
    "    Wo += np.dot(l1.T, l2_delta)\n",
    "    Wh += np.dot(X.T, l1_delta)\n",
    "\n",
    "print(\"Wo:\\n\", Wo)\n",
    "print(\"Wh:\\n\", Wh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (WIP) Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Reproducing Rosenblatt's NYT Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Times_July_13_Rosenblatt_Perceptron](resources/Times_July_13_Rosenblatt_Perceptron.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
