{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A perceptron is simple model that can help us solve such a problem given enough data. **The perceptron can be defined as an algorithm for supervised binary classification.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's on the agenda: \n",
    "\n",
    "- We will provide a case study on the Perceptron.\n",
    "- We will perform inference on the Perceptron.\n",
    "- We will training the perceptron with data\n",
    "- Scaling the perceptron: Multi-Layer Perceptron\n",
    "- In-class assignment which will include building your own MLP model on data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study: Mbali's Gym Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine we have to predict whether your friend, Mbali, would be going for a workout or not. We have the following things to take into account:\n",
    "\n",
    "- Weather (Sunny or Rainy) ?\n",
    "- Time of Day (Morning or Evening) ?\n",
    "- Energy Level (Energized) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/perceptron_cartoon.png\" style=\"max-width:100%; width: 70%; max-width: none\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons are made up of three components, namely:\n",
    "\n",
    "   - The input ($x_k$) : For example, the current status of the weather, Mbali's energy level, the time of day.\n",
    "   - The input weights ($w_k$) : For example, how to weight the contribution of Mbali's energy towards the final decision (Gym or No Gym)\n",
    "   - The activation function ($\\sigma$) : For example, a decision function which integrates the inputs to product the final decision (Gym or No Gym)\n",
    "    \n",
    "Mathematically, this translates to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y = \\sigma( \\sum_k^{N} w_k x_k + b) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's dig into building this perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prelim: Let's make sure our packages work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To make the outputs deterministic, we seed the random number generator with a constant. This will guarantee that every time you run the code, you will get the same random distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start building a perceptron!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a framework (class) to create perceptrons. With this class, we can create how ever many perceptrons we want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a perceptron class which will compute the **dot-product** $w \\cdot x$ and add the **bias** $b$ to it. Subsequently, the activation function will convert this **dot-product** to the **activation value** $a$ of the unit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, weights, bias, activation):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        \n",
    "    def predict(self, x):\n",
    "        pre_activation = self.weights.dot(x) + self.bias\n",
    "        return self.activation(pre_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define our example data **input** $x$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = 0\n",
    "evening = 1\n",
    "energized = 1\n",
    "x = np.array([weather, evening, energized])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the corresponding **weights** $w$, and **bias** $b$ of our perceptron $p$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([0.5, 0.5, 0])\n",
    "bias = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, let's now define our *activation* function. We choose this binary activation function, but we are free to choose any activation function of our choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_activation_fn(z):\n",
    "    if z == 0.5:\n",
    "        return 1\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create our first perceptron object with our corresponding **weights**, **bias** and **activation function**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron(weights, bias, binary_activation_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make our first prediction with $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = perceptron.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation Value of Perceptron (a): 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Activation Value of Perceptron (a):\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Assignments\n",
    "\n",
    "Provide an instance $x$ to the perceptron $p$, where your friend, Mbali, does not feel energized, while it's the morning and it's raining?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an activation function with a threshold of $0.5$, and now provide an input to the perceptron for when, Mbali, feels energized while it's raining outside in the evening.\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "    \\ 0 & \\quad \\text{if } w \\cdot x + b \\leq 0.5\\\\\n",
    "    \\ 1 & \\quad \\text{if } w \\cdot x + b > 0.5\n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the same function, play with bias values = $[-1, -0.5, 0, 0.5, 1]$ and observe how that changes the output of the perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Logic Gates (AND, OR) with Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logic gates are one of the first applications of the perceptrons and have been heavily explored in early literature. Here we get the opportunity to explore some of the early research questions in this field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logic gates are boolean functions $f:\\{0,1\\}^k \\rightarrow \\{0,1\\}$, that have applications in electronic gates and mathematical logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons are thought to be good function approximators for Logic Gates. In this mini-tutorial, we will fit logic gates, AND & OR, with a perceptron!\n",
    "\n",
    "| x1 | x2  | AND | OR |\n",
    "| --- | ------ |:------:| ---- |\n",
    "| 0 | 0  | 0 | 0 |\n",
    "| 0 | 1  | 0 | 1 |\n",
    "| 1 | 0  | 0 | 1 |\n",
    "| 1 | 1  | 1 | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to implement a simple **perceptron** to compute logical operations like AND, and OR.\n",
    "\n",
    "- Input: $x_1$ and $x_2$\n",
    "- Bias: $b = -1$ for AND; $b = 0$ for OR\n",
    "- Weights: $w = [1, 1]$\n",
    "\n",
    "with the following activation function:\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "    \\ 0 & \\quad \\text{if } w \\cdot x + b \\leq 0\\\\\n",
    "    \\ 1 & \\quad \\text{if } w \\cdot x + b > 0\n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0]\n",
    "    ,[1,0]\n",
    "    ,[0,1]\n",
    "    ,[1,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define this threshold function in Python as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(z):\n",
    "    if z > 0:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the AND operation, we implement the perceptron as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AND 0: 0\n",
      "1 AND 0: 0\n",
      "0 AND 1: 0\n",
      "1 AND 1: 1\n"
     ]
    }
   ],
   "source": [
    "w = np.array([1, 1])\n",
    "b = -1\n",
    "\n",
    "perceptron = Perceptron(w, b, activation)\n",
    "\n",
    "for x in X:\n",
    "    print(f'{x[0]} AND {x[1]}: {perceptron.predict(x)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the OR operation, we implement the perceptron as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 OR 0: 0\n",
      "1 OR 0: 1\n",
      "0 OR 1: 1\n",
      "1 OR 1: 1\n"
     ]
    }
   ],
   "source": [
    "w = np.array([1, 1])\n",
    "b = 0\n",
    "\n",
    "perceptron = Perceptron(w, b, activation)\n",
    "\n",
    "for x in X:\n",
    "    print(f'{x[0]} OR {x[1]}: {perceptron.predict(x)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Class Exercise\n",
    "\n",
    "Can you implement a perceptron that computes 2 logic gates for the NOR operation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| x1 | x2  | NOR |\n",
    "| --- | ------ |:------:|\n",
    "| 0 | 0  | 1 |\n",
    "| 0 | 1  | 0 |\n",
    "| 1 | 0  | 0 |\n",
    "| 1 | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Perceptrons: Gradient Descent and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To teach the perceptron to recognize pictures of cats ***(for example)***, we show it a bunch of pictures and tell it whether each picture is a cat or not. The perceptron tries to guess whether each picture is a cat or not based on the numbers it receives, but it doesn't always get it right.\n",
    "\n",
    "***The \"loss function\" is like a scorekeeper that tells you how well the perceptron is doing at recognizing cats.*** Every time the perceptron makes a mistake, the scorekeeper gives it a \"point\" (or a \"loss\"). Your goal is to make the scorekeeper's score as low as possible by adjusting the perceptron's \"weights\". The weights are like the settings inside the perceptron that determine how it works. By minimizing the scorekeeper's score, you can teach the perceptron to become better at recognizing cats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update the weights of a perceptron $p$ given some data $x$ and loss function $L$, we use ***Gradient Descent***.\n",
    "\n",
    "***Gradient Descent*** is a first-order gradient optimization algorithm. At first, the weights are randomly initialized, then the optimization algorithm takes repeated steps in the opposite direction of the gradient until it reaches a \"satisfactory condition\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SGD](resources/sgd.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent equation can be encapsulated in the following equation:\n",
    "\n",
    "$$\n",
    "w_{k+1} = w_{k} - \\alpha \\nabla L_{w}\n",
    "$$\n",
    "\n",
    ",where $L$ is the loss function and $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Backpropagation*** computes the gradient of a loss function with respect to the weights of the network $\\nabla L_{w}$ for a single inputâ€“output example, and does so efficiently, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule.\n",
    "\n",
    "Let's first define a simple loss function $L(w)$ for a perceptron $p$:\n",
    "\n",
    "$$\n",
    "L(w) = (target - y)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = \\sigma( \\sum_k^{N} w_k x_k + b) \n",
    "$$\n",
    "\n",
    "Compute the gradient for a perceptron, takes the following shape:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_i} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial \\sigma} \\frac{\\partial \\sigma}{\\partial w_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we fit a simple logic problem. We want to make the following predictions from the input:\n",
    "\n",
    "| x1 | x2 | x3 | Output |\n",
    "| - | - | - |:------:|\n",
    "| 0 | 0 | 1  | 0      |\n",
    "| 1 | 1 | 1  | 1      |\n",
    "| 1 | 0 | 1  | 1      |\n",
    "| 0 | 1 | 1  | 0      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now, we have been looking at linear activation functions, mostly threshold functions. To appromixate more interesting functions, we will need to look into non-linear activation functions, namely the Sigmoid and ReLU activation functions,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the *[Sigmoid](http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#sigmoid)* activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use the [ReLU](http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#activation-relu) activation function instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \"\"\"The ReLU activation function.\"\"\"\n",
    "    return max(0, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Sigmoid](http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#sigmoid) activation function introduces non-linearity to the computation. It maps the input value to an output value between $0$ and $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/SigmoidFunction1.png\" style=\"max-width:100%; width: 30%; max-width: none\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the sigmoid function is maximal at $x=0$ and minimal for lower or higher values of $x$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/sigmoid_prime.png\" style=\"max-width:100%; width: 25%; max-width: none\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *sigmoid_prime* function returns the derivative of the sigmoid for any given $z$. The derivative of the sigmoid is $z * (1 - z)$. This is basically the slope of the sigmoid function at any given point: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    \"\"\"The derivative of sigmoid for z.\"\"\"\n",
    "    return z * (1 - z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Class Exercise\n",
    "\n",
    "What is the derivative of the ReLU activation function? Build a function for the ReLU derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the inputs as rows in *X*. There are three input nodes (three columns per vector in $X$. Each row is one trainig example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([ [ 0, 0, 1 ],\n",
    "               [ 0, 1, 1 ],\n",
    "               [ 1, 0, 1 ],\n",
    "               [ 1, 1, 1 ] ])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs are stored in *y*, where each row represents the output for the corresponding input vector (row) in *X*. The vector is initiated as a single row vector and with four columns and transposed (using the $.T$ method) into a column vector with four rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "y = np.array([[0,0,1,1]]).T\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our perceptron, we create a weight matrix ($Wo$) with randomly initialized weights:\n",
    "\n",
    "TODO: Delve into how we initialize the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.43532763]\n",
      " [0.5649153 ]\n",
      " [0.25761743]]\n"
     ]
    }
   ],
   "source": [
    "n_inputs = 3\n",
    "n_outputs = 1\n",
    "Wo = np.random.random( (n_inputs, n_outputs) ) * np.sqrt(2.0/n_inputs)\n",
    "print(Wo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for the output weight matrix ($Wo$) to have 3 rows and 1 column is that it represents the weights of the connections from the three input neurons to the single output neuron. The initialization of the weight matrix is random with a mean of $0$ and a variance of $1$. There is a good reason for chosing a mean of zero in the weight initialization. See for details the section on Weight Initialization in the [Stanford course CS231n on Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/neural-networks-2/#init).\n",
    "\n",
    "\n",
    "The core representation of this network is basically the weight matrix *Wo*. The rest, input matrix, output vector and so on are components that we need for learning and evaluation. The learning result is stored in the *Wo* weight matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop in the optimization and learning cycle 10,000 times. In the *forward propagation* line we process the entire input matrix for training. This is called **full batch** training. I do not use an alternative variable name to represent the input layer, instead I use the input matrix $X$ directly here. Think of this as the different inputs to the input neurons computed at once. In principle the input or training data could have many more training examples, the code would stay the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the result of the perceptron without training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56405051],\n",
       "       [0.6947737 ],\n",
       "       [0.66662175],\n",
       "       [0.77865756]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.dot(X, Wo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recall that the loss function is the squared error loss and using the perceptron gradient equation, our backpropagation equation would translate to:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_i} = [ - 2 * (target - y)] * [\\sigma'(\\sum_k^{N} w_k x_k + b)] * [x_i] \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training with this gradient update equation, we will improve error on our perceptron model. \n",
    "\n",
    "Observe the error of our initialized perceptron model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31815298],\n",
       "       [0.4827105 ],\n",
       "       [0.11114106],\n",
       "       [0.04899248]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y - sigmoid(np.dot(X, Wo)))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(10000):\n",
    "    \n",
    "    # forward propagation\n",
    "    prediction = sigmoid(np.dot(X, Wo))\n",
    "    \n",
    "    # compute the loss\n",
    "    loss_gradient = -2 * (y - prediction)\n",
    "    \n",
    "    # multiply the loss by the slope of the sigmoid at l1 (Backpropagation Step)\n",
    "    l1_delta = sigmoid_prime(prediction) * loss_gradient\n",
    "    gradients = np.dot(X.T, l1_delta)\n",
    "    \n",
    "    # update weights (Gradient Descent)\n",
    "    Wo += - gradients\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now observe the error of our trained perceptron model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.61951671e-05],\n",
       "       [3.06376089e-05],\n",
       "       [2.03748845e-05],\n",
       "       [3.07354915e-05]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y - sigmoid(np.dot(X, Wo)))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the result of the perceptron with training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0067967 ],\n",
       "       [0.00553513],\n",
       "       [0.99548615],\n",
       "       [0.99445604]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.dot(X, Wo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations of the Perceptron: The Famous XOR Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of neural units comes from combining them into larger networks. Minsky and Papert (1969): A single neural unit cannot compute the simple logical function XOR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this narrow definition of a perceptron, it seems not possible to implement an XOR logic perceptron. The restriction is that there is a threshold function that is binary and piecewise linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a way to fit the XOR logic gate with a perceptron. If we use the following activation function:\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "    \\ 0 & \\quad \\text{if } w \\cdot x + b \\neq 0.5\\\\\n",
    "    \\ 1 & \\quad \\text{if } w \\cdot x + b = 0.5\n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The only problem is that it is non-differentiable, so we can't use gradient descent.\n",
    "\n",
    "We illustrate this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bactivation(z):\n",
    "    if z == 0.5:\n",
    "        return 1\n",
    "    else: return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume the weights to be set to 0.5 and the bias to 0, one unit can handle the XOR logic:\n",
    "\n",
    "- Input: $x_1$ and $x_2$\n",
    "- Bias: $b = 0$ for XOR\n",
    "- Weights: $w = [0.5, 0.5]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 OR 0: 0\n",
      "1 OR 0: 1\n",
      "0 OR 1: 1\n",
      "1 OR 1: 0\n"
     ]
    }
   ],
   "source": [
    "w = np.array([0.5, 0.5])\n",
    "b = 0\n",
    "x = np.array([0, 0])\n",
    "print(\"0 OR 0:\", bactivation(w.dot(x) + b))\n",
    "x = np.array([1, 0])\n",
    "print(\"1 OR 0:\", bactivation(w.dot(x) + b))\n",
    "x = np.array([0, 1])\n",
    "print(\"0 OR 1:\", bactivation(w.dot(x) + b))\n",
    "x = np.array([1, 1])\n",
    "print(\"1 OR 1:\", bactivation(w.dot(x) + b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interesting background knowledge on the XOR problem plus a fun fact!\n",
    "\n",
    "This particular activation function is of course not differentiable, and it remains to be shown that the weights can be learned, but nevertheless, a single unit can be identified that solves the XOR problem.\n",
    "\n",
    "The difference between Minsky and Papert's (1969) definition of a perceptron and this unit is that - as Julia Hockenmaier pointed out - a perceptron is defined to have a decision function that would be binary and piecewise linear. This means that the unit that solves the XOR problem is not compatible with the definition of perceptron as in Minsky and Papert (1969) (p.c. Julia Hockenmaier).\n",
    "\n",
    "Minsky and Papert's (1969) claims on the XOR problem was one of the contributing factors towards the famous AI Winter: A period of reduced funding towards Artificial Intelligence research.\n",
    "\n",
    "Also, Seymour Papert is from South Africa!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the XOR Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a proposed solution in [Goodfellow et al. (2016)](https://www.deeplearningbook.org/) for the XOR problem, using a network with two layers of ReLU-based units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![XOR Network](resources/XOR_Network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This two layer and three perceptron network solves the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will show why the multi-layer perceptron allows us to solve more complex problems like the XOR problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Multi-Layer Perceptron is a model which consists of at least three layers of perceptrons: input layer, hidden layer, and output layer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLP](resources/mlp_hidden.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a viz that shows that multi-layer perceptrons are better for fitting data vs single-layer perceptrons. Universal function approximator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would backpropagation work in such a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLP_backprop](resources/mlp_backprop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following dataset:\n",
    "\n",
    "| Input  | Output |\n",
    "| ------ |:------:|\n",
    "| 0 0 1  | 0      |\n",
    "| 0 1 1  | 1      |\n",
    "| 1 0 1  | 1      |\n",
    "| 1 1 1  | 0      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern here is a XOR pattern problem: If there is a $1$ in either column $1$ or $2$, but not in both, the output is $1$ (XOR over column $1$ and $2$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this problem, we need a network with another layer (MLP), that is a layer that will combine and transform the input, and an additional layer will map it to the output. We will add a *hidden layer* with randomized weights and then train those to optimize the output probabilities of the table above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a new $X$ input matrix that reflects the above table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0, 1],\n",
    "              [0, 1, 1],\n",
    "              [1, 0, 1],\n",
    "              [1, 1, 1]])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a new output matrix $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "y = np.array([[ 0, 1, 1, 0]]).T\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the random number generator with a constant again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that our 3 inputs are mapped to 4 hidden layer ($Wh$) neurons, we have to initialize the hidden layer weights in a 3 by 4 matrix. The outout layer ($Wo$) is a single neuron that is connected to the hidden layer, thus the output layer is a 4 by 1 matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wh:\n",
      " [[0.54642637 0.21630591 0.05416217 0.30217248]\n",
      " [0.51416219 0.17160636 0.61462234 0.05432681]\n",
      " [0.21254639 0.65707935 0.15793843 0.52211762]]\n",
      "Wo:\n",
      " [[0.37099793]\n",
      " [0.65393799]\n",
      " [0.18617893]\n",
      " [0.04664153]]\n"
     ]
    }
   ],
   "source": [
    "n_inputs = 3\n",
    "n_hidden_neurons = 4\n",
    "n_output_neurons = 1\n",
    "Wh = np.random.random( (n_inputs, n_hidden_neurons) )  * np.sqrt(2.0/n_inputs)\n",
    "Wo = np.random.random( (n_hidden_neurons, n_output_neurons) )  * np.sqrt(2.0/n_hidden_neurons)\n",
    "print(\"Wh:\\n\", Wh)\n",
    "print(\"Wo:\\n\", Wo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the current initial prediction of the Multi-Layer Perceptron (MLP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.68255195],\n",
       "       [0.70318206],\n",
       "       [0.7004341 ],\n",
       "       [0.71831433]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.dot(sigmoid(np.dot(X, Wh)), Wo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will loop now 100,000 times to optimize the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.003887504994617163\n",
      "Error: 0.0036937834970836095\n",
      "Error: 0.003525709960695171\n",
      "Error: 0.003378132302446337\n",
      "Error: 0.003247233371050477\n",
      "Error: 0.003130117979719035\n",
      "Error: 0.003024545801488532\n",
      "Error: 0.0029287529741954285\n",
      "Error: 0.0028413296155250386\n",
      "Error: 0.0027611336786237985\n"
     ]
    }
   ],
   "source": [
    "for i in range(100000):\n",
    "    # forward propagation\n",
    "    l1 = sigmoid(np.dot(X, Wh))\n",
    "    l2 = sigmoid(np.dot(l1, Wo))\n",
    "    \n",
    "    l2_error = y - l2\n",
    "    \n",
    "    if (i % 10000) == 0:\n",
    "        print(\"Error:\", np.mean(np.abs(l2_error)))\n",
    "    \n",
    "    # gradient of output layer with respect to loss\n",
    "    l2_delta = l2_error * sigmoid_prime(l2)\n",
    "    \n",
    "    # compute the l1 contribution by value to the l2 error\n",
    "    l1_error = l2_delta.dot(Wo.T)\n",
    "    \n",
    "    # gradient of hidden layer with respect to loss\n",
    "    l1_delta = l1_error * sigmoid_prime(l1)\n",
    "    \n",
    "    Wo += np.dot(l1.T, l2_delta)\n",
    "    Wh += np.dot(X.T, l1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the trained prediction of the Multi-Layer Perceptron (MLP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.54277709e-04],\n",
       "       [9.95491443e-01],\n",
       "       [9.95479782e-01],\n",
       "       [5.56696699e-03]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.dot(sigmoid(np.dot(X, Wh)), Wo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project Idea:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the original July 13, 1958 New York Times article describing Rosenblatt's perceptron.\n",
    "\n",
    "As a mini-project, you could consider reproducing the \"square-locating\" task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/Times_July_13_Rosenblatt_Perceptron.png\" style=\"max-width:100%; width: 70%; max-width: none\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
